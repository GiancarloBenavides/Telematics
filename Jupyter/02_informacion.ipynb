{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La teoría de la información\n",
    "<p><code>Python en Jupyter Notebook</code></p>\n",
    "<p>Creado por <code>Giancarlo Ortiz</code> para el curso de <code>Redes</code></p>\n",
    "\n",
    "## La información\n",
    "La información es una propiedad concreta de la materia y la energía que es cuantificable y mensurable.\n",
    "\n",
    "### Agenda\n",
    "1. Incertidumbre\n",
    "1. Información\n",
    "1. Entropía\n",
    "1. Codificación de la fuente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar módulos al cuaderno\n",
    "import math as m\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats as st\n",
    "\n",
    "# Definir e incluir nuevas funciones al cuaderno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Incertidumbre:\n",
    "---\n",
    "Falta de confianza o certeza sobre un evento, en el caso de un alfabeto de entrada $X = \\{x_1, x_2,...,x_i,...,x_n\\}$ de $n$ símbolos con probabilidades de ocurrencia $P = \\{p_1,...,p_n\\}$ antes del primer símbolo y probabilidades de ocurrencia $\\tilde P = \\{ \\tilde p_1,..., \\tilde p_n\\}$ antes del segundo símbolo; la incertidumbre es proporcional a la sumatoria de la probabilidad de ocurrencia de todos los símbolos posibles; esto es: \n",
    "\n",
    "\\begin{align*}\n",
    "S =\n",
    "\\begin{cases}\n",
    "S_{0}=-\\sum _{i=1}^{n}p_{i}\\log _{2}p_{i} \\quad (\\text{Incertidumbre inicial}) \\\\\n",
    "S_{f}=-\\sum _{i=1}^{n}{\\tilde {p}}_{i}\\log _{2}{\\tilde {p}}_{i} \\quad (\\text{Incertidumbre final})\\\\\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    ">Nota:\n",
    "> * Si la base del logaritmo es $b=2$, la unidad de información es __bits__;\n",
    "> * Si la base es $b=e$, entonces la unidad es __nats__; \n",
    "> * Y finalmente si $b=10$, la unidad es __hartleys__.\n",
    "\n",
    "### <code>Ejemplo:</code> Alfabeto español\n",
    "Si se desea transmitir un mensaje binario escrito originalmente usando las mayúsculas del alfabeto latino y según la [frecuencia de aparición](https://es.wikipedia.org/wiki/Frecuencia_de_aparici%C3%B3n_de_letras) de letras en el español:\n",
    "\n",
    "* ¿Cuál es el número de símbolos del alfabeto latino?\n",
    "* ¿Cuál es el tamaño mínimo de bits para transmitir un símbolo?\n",
    "* ¿Cuál es la suma de todas las probabilidades de símbolo?\n",
    "* ¿Cuál es la probabilidad de una vocal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de símbolos posibles:  27\n",
      "Tamaño de palabra:  5 bits\n",
      "Suma de probabilidades letras:  1.0\n",
      "Suma de probabilidades vocales:  0.45\n"
     ]
    }
   ],
   "source": [
    "# Matriz de probabilidades\n",
    "probabilidades = {\n",
    "    \"A\": 0.1253, \"B\": 0.0142, \"C\": 0.0468, \"D\": 0.0586, \"E\": 0.1368, \"F\": 0.0069,\n",
    "    \"G\": 0.0101, \"H\": 0.0070, \"I\": 0.0625, \"J\": 0.0044, \"K\": 0.0001, \"L\": 0.0497,\n",
    "    \"M\": 0.0315, \"N\": 0.0671, \"Ñ\": 0.0031, \"O\": 0.0868, \"P\": 0.0251, \"Q\": 0.0088,\n",
    "    \"R\": 0.0687, \"S\": 0.0798, \"T\": 0.0463, \"U\": 0.0393, \"V\": 0.0090, \"W\": 0.0001,\n",
    "    \"X\": 0.0022, \"Y\": 0.0090, \"Z\": 0.0052\n",
    "}\n",
    "\n",
    "# Probabilidades de ocurrencia por letra\n",
    "probabilidad_letras = probabilidades.values()\n",
    "\n",
    "# Probabilidades de ocurrencia de las vocales\n",
    "probabilidad_vocales = [\n",
    "    probabilidades[\"A\"],\n",
    "    probabilidades[\"E\"],\n",
    "    probabilidades[\"I\"],\n",
    "    probabilidades[\"O\"],\n",
    "    probabilidades[\"U\"]\n",
    "]\n",
    "\n",
    "# Número de símbolos en el alfabeto\n",
    "letras = len(probabilidad_letras)\n",
    "# Tamaño mínimo de palabra (binario)\n",
    "tamaño = round(m.log2(letras))\n",
    "# Sumatoria de probabilidades\n",
    "suma_letras = round(sum(probabilidad_letras), 2)\n",
    "# Sumatoria de probabilidades para una vocal\n",
    "suma_vocales = round(sum(probabilidad_vocales), 2)\n",
    "\n",
    "# Salida\n",
    "print(\"Número de símbolos posibles: \", letras)\n",
    "print(\"Tamaño de palabra: \", tamaño, \"bits\")\n",
    "print(\"Suma de probabilidades letras: \", suma_letras)\n",
    "print(\"Suma de probabilidades vocales: \", suma_vocales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <code>Ejemplo:</code> Alfabeto español\n",
    "Continuando con el ejemplo anterior:\n",
    "\n",
    "* ¿Cuál es la incertidumbre de un símbolo para una palabra nueva?\n",
    "* ¿Cuál es la incertidumbre de el segundo símbolo si el primero en llegar es una \"Z\"?\n",
    "* ¿Cuál es el cambio en la incertidumbre de símbolo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| -------------------------------------- |\n",
      "| Pos | letra | Inicial | Final | Cambio |\n",
      "| -------------------------------------- |\n",
      "|  1  |   Z   |    4.04 |  1.51 |   2.53 |\n",
      "| -------------------------------------- |\n"
     ]
    }
   ],
   "source": [
    "# Probabilidades primera letra\n",
    "P1 = probabilidad_letras\n",
    "P2 = probabilidad_vocales\n",
    "\n",
    "s0, s1 = 0, 0\n",
    "for item in P1:\n",
    "    s0 -= item * m.log2(item)\n",
    "\n",
    "for item in P2:\n",
    "    s1 -= item * m.log2(item)\n",
    "\n",
    "# Cambio en la incertidumbre\n",
    "Δs = - (s1 - s0)\n",
    "\n",
    "# Salida\n",
    "print(f\"|\", \"-\"*38, \"|\")\n",
    "print(f\"| Pos | letra | Inicial | Final | Cambio |\")\n",
    "print(f\"|\", \"-\"*38, \"|\")\n",
    "print(f\"|  1  |   Z   | {s0:7.2f} | {s1:5.2f} | {Δs:6.2f} |\")\n",
    "print(f\"|\", \"-\"*38, \"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Información\n",
    "---\n",
    "### 2.1. Definición:\n",
    "Es un conjunto de datos (mensaje) sobre un determinado fenómeno que no se conocían.\n",
    "\n",
    "Se basa en:\n",
    "* La información es transferida desde una fuente a un destinatario, solo si este ultimo no la conocía previamente.\n",
    "* La información siempre es mayor o igual a cero\n",
    "* No existe información en un símbolo seguro (probabilidad)\n",
    "* Existe información infinita en un símbolo imposible (probabilidad)\n",
    "\n",
    "### 2.2. Ecuaciones:\n",
    "\n",
    "\\begin{align*}\n",
    "I(x_i) &= - (S_f - S_o) \\quad \\text{Reducción de la incertidumbre} \\\\\n",
    "I(x_i) &= log_b \\frac {1}{p(x_i)} = - log_b p(x_i) \\quad \\text{Codificador optimo} \\\\\n",
    "\\end{align*}\n",
    "\n",
    ">Nota:\n",
    "> * Si la base del logaritmo es $b=2$, la unidad de información es __bits__;\n",
    "> * Si la base es $b=e$, entonces la unidad es __nats__; \n",
    "> * Y finalmente si $b=10$, la unidad es __hartleys__.\n",
    "\n",
    "\n",
    "### <code>Ejemplo:</code> Alfabeto español\n",
    "Continuando con la distribución de probabilidades del ejemplo anterior:\n",
    "\n",
    "* ¿Cuál es la información asociada a la primera letra \"Z\" en un codificador optimo?\n",
    "* ¿Cuál es la información asociada a la llegada de la primera letra \"Z\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La información asociada a la letra Z en un codificador optimo:   7.59 bits.\n",
      "La información asociada a la letra Z es:   2.53 bits.\n"
     ]
    }
   ],
   "source": [
    "info_optimo = m.log2(1/probabilidades[\"Z\"])\n",
    "\n",
    "# Salida\n",
    "print(f\"La información asociada a la letra Z en un codificador optimo es: {info_optimo:6.2f} bits.\")\n",
    "print(f\"La información asociada a la letra Z en s: {Δs:6.2f} bits.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Entropía\n",
    "---\n",
    "### 1.1. Definición:\n",
    "En teoría de la información la entropía es una medida de la incertidumbre de una fuente de información.\n",
    "\n",
    "Se basa en:\n",
    "* la entropía es proporcional a la información \n",
    "* Si todos los símbolos de una fuente son igual de probables, entonces la entropía de la fuente será máxima.\n",
    "\n",
    "### 1.2. Principios:\n",
    "Como consecuencia directa de esta definición se puede expresar que dada una variable aleatoria A esta debe cumplir los siguientes principios:\n",
    "\n",
    "\\begin{align*}\n",
    "Prob(A) = P(A) &\\Rightarrow 0 ≤ P(A) ≤1  \\\\\n",
    "B = !A = \\bar A &\\Rightarrow P(B) = 1 − P(A) \\\\\n",
    "P(A o B) = P(A+B) &\\Rightarrow P(A+B) = P(A∪B) \\\\\n",
    "P(A y B)=P(AB) &\\Rightarrow P(AB) = P(A∩B) \\\\\n",
    "P(A si B)=P(A/B) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "### 1.3. Propiedades:\n",
    "Algunas de las reglas que podemos mencionar:\n",
    "\n",
    "| Nombre | Ecuación | Nota |\n",
    "|--|--|:--:|\n",
    "| Numero de aciertos | $ P(A) = \\lim\\limits_{n \\to \\infty} \\frac{a}{n} $ | (1) |\n",
    "| Probabilidad a-priori | $ P(A) = \\frac{c}{n} $ | (2) |\n",
    "| Regla de la adición | $ P(A+B)=P(A)+P(B)−P(AB) $ |\n",
    "| Regla de la multiplicación | $ P(AB)=P(A/B)P(B) = P(B/A)P(A) $ |\n",
    "| Regla de la exclusión | $ P(AB)=0 \\Rightarrow P(A+B)=P(A)+P(B) $ |\n",
    "| Teorema de Bayes | $ P(A/B)= \\frac{P(AB)}{P(B)}=P(B/A) ∗  \\frac{P(A)}{P(B)} $ |\n",
    "\n",
    "<p><small>\n",
    "<div><sup>1</sup> n = número de intentos, a = aciertos </div>\n",
    "<div><sup>2</sup> n = número de casos, c = coincidencias</div>\n",
    "</small></p>\n",
    "\n",
    "### <code>Ejemplo:</code> caja de laboratorio con resistencias\n",
    "Una caja oscura de laboratorio contiene 20 resistencias, 5 resistencias de 100Ω y 15 resistencias de 50Ω. De la caja se extraen dos resistencias, una después de otra sin reponer la primera, determinar las siguientes probabilidades:\n",
    "\n",
    "Probabilidades tomando una resistencia de la caja (a-priori):\n",
    "* La probabilidad de que las primera resistencia sea de 100Ω\n",
    "* La probabilidad de que las primera resistencia sea de 50Ω\n",
    "\n",
    "Probabilidades al tomar la segunda resistencia:\n",
    "* La probabilidad de que las dos resistencias sean de 100Ω\n",
    "* La probabilidad de que las dos resistencias son de 50Ω\n",
    "* La probabilidad de que la primera fue de 100Ω y la segunda de 50Ω\n",
    "* La probabilidad de que la primera fue de 50Ω y la segunda de 100Ω"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Mas Recursos\n",
    "\n",
    "- [Información](https://es.wikipedia.org/wiki/Informaci%C3%B3n) (Wikipedia)\n",
    "- [Entropía](https://es.wikipedia.org/wiki/Entrop%C3%ADa_(informaci%C3%B3n)) (Wikipedia)\n",
    "- [Teoría de la información](https://es.wikipedia.org/wiki/Teor%C3%ADa_de_la_informaci%C3%B3n) (Wikipedia)\n",
    "- [Ley de los grandes números](https://es.wikipedia.org/wiki/Ley_de_los_grandes_n%C3%BAmeros) (Wikipedia)\n",
    "- [Teorema del límite central](https://es.wikipedia.org/wiki/Teorema_del_l%C3%ADmite_central) (Wikipedia)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
