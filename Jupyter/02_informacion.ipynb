{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La teoría de la información\n",
    "<p><code>Python en Jupyter Notebook</code></p>\n",
    "<p>Creado por <code>Giancarlo Ortiz</code> para el curso de <code>Redes</code></p>\n",
    "\n",
    "## La información\n",
    "La información es una propiedad concreta de la materia y la energía que es cuantificable y mensurable.\n",
    "\n",
    "### Agenda\n",
    "1. Incertidumbre\n",
    "1. Información\n",
    "1. Entropía\n",
    "1. Codificación de la fuente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar módulos al cuaderno\n",
    "import math as m\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats as st\n",
    "\n",
    "# Definir e incluir nuevas funciones al cuaderno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Incertidumbre:\n",
    "---\n",
    "Falta de confianza o certeza sobre un evento, en el caso de un alfabeto de entrada $X = \\{x_1, x_2,...,x_i,...,x_n\\}$ de $n$ símbolos con probabilidades de ocurrencia $P = \\{p_1,...,p_n\\}$ antes del primer símbolo y probabilidades de ocurrencia $\\tilde P = \\{ \\tilde p_1,..., \\tilde p_n\\}$ antes del segundo símbolo; la incertidumbre es proporcional a la sumatoria de la probabilidad de ocurrencia de todos los símbolos posibles; esto es: \n",
    "\n",
    "\\begin{align*}\n",
    "S =\n",
    "\\begin{cases}\n",
    "S_{0}=-\\sum _{i=1}^{n}p_{i}\\log _{2}p_{i} \\quad (\\text{Incertidumbre inicial}) \\\\\n",
    "S_{f}=-\\sum _{i=1}^{n}{\\tilde {p}}_{i}\\log _{2}{\\tilde {p}}_{i} \\quad (\\text{Incertidumbre final})\\\\\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    ">Nota:\n",
    "> * Si la base del logaritmo es $b=2$, la unidad de información es __bits__;\n",
    "> * Si la base es $b=e$, entonces la unidad es __nats__; \n",
    "> * Y finalmente si $b=10$, la unidad es __hartleys__.\n",
    "\n",
    "### <code>Ejemplo:</code> Alfabeto español\n",
    "Si se desea transmitir un mensaje binario escrito originalmente usando las mayúsculas del alfabeto latino y según la [frecuencia de aparición](https://es.wikipedia.org/wiki/Frecuencia_de_aparici%C3%B3n_de_letras) de letras en el español:\n",
    "\n",
    "* ¿Cuál es el número de símbolos del alfabeto latino?\n",
    "* ¿Cuál es el tamaño mínimo de bits para transmitir un símbolo?\n",
    "* ¿Cuál es la suma de todas las probabilidades de símbolo?\n",
    "* ¿Cuál es la probabilidad de una vocal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de símbolos posibles:  27\n",
      "Tamaño de palabra:  5 bits\n",
      "Suma de probabilidades letras:  1.0\n",
      "Suma de probabilidades vocales:  0.45\n"
     ]
    }
   ],
   "source": [
    "# Matriz de probabilidades\n",
    "probabilidades = {\n",
    "    \"A\": 0.1253, \"B\": 0.0142, \"C\": 0.0468, \"D\": 0.0586, \"E\": 0.1368, \"F\": 0.0069,\n",
    "    \"G\": 0.0101, \"H\": 0.0070, \"I\": 0.0625, \"J\": 0.0044, \"K\": 0.0001, \"L\": 0.0497,\n",
    "    \"M\": 0.0315, \"N\": 0.0671, \"Ñ\": 0.0031, \"O\": 0.0868, \"P\": 0.0251, \"Q\": 0.0088,\n",
    "    \"R\": 0.0687, \"S\": 0.0798, \"T\": 0.0463, \"U\": 0.0393, \"V\": 0.0090, \"W\": 0.0001,\n",
    "    \"X\": 0.0022, \"Y\": 0.0090, \"Z\": 0.0052\n",
    "}\n",
    "\n",
    "# Probabilidades de ocurrencia por letra\n",
    "probabilidad_letras = probabilidades.values()\n",
    "\n",
    "# Probabilidades de ocurrencia de las vocales\n",
    "probabilidad_vocales = [\n",
    "    probabilidades[\"A\"],\n",
    "    probabilidades[\"E\"],\n",
    "    probabilidades[\"I\"],\n",
    "    probabilidades[\"O\"],\n",
    "    probabilidades[\"U\"]\n",
    "]\n",
    "\n",
    "# Número de símbolos en el alfabeto\n",
    "letras = len(probabilidad_letras)\n",
    "# Tamaño mínimo de palabra (binario)\n",
    "tamaño = round(m.log2(letras))\n",
    "# Sumatoria de probabilidades\n",
    "suma_letras = round(sum(probabilidad_letras), 2)\n",
    "# Sumatoria de probabilidades para una vocal\n",
    "suma_vocales = round(sum(probabilidad_vocales), 2)\n",
    "\n",
    "# Salida\n",
    "print(\"Número de símbolos posibles: \", letras)\n",
    "print(\"Tamaño de palabra: \", tamaño, \"bits\")\n",
    "print(\"Suma de probabilidades letras: \", suma_letras)\n",
    "print(\"Suma de probabilidades vocales: \", suma_vocales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <code>Ejemplo:</code> Alfabeto español\n",
    "Continuando con el ejemplo anterior:\n",
    "\n",
    "* ¿Cuál es la incertidumbre de un símbolo para una palabra nueva?\n",
    "* ¿Cuál es la incertidumbre de el segundo símbolo si el primero en llegar es una \"Z\"?\n",
    "* ¿Cuál es el cambio en la incertidumbre de símbolo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| -------------------------------------- |\n",
      "| Pos | letra | Inicial | Final | Cambio |\n",
      "| -------------------------------------- |\n",
      "|  1  |   Z   |    4.04 |  2.20 |   1.84 |\n",
      "| -------------------------------------- |\n"
     ]
    }
   ],
   "source": [
    "# Probabilidades primera letra\n",
    "P1 = probabilidad_letras\n",
    "P2 = list(np.multiply(np.array(probabilidad_vocales), 1/suma_vocales))\n",
    "\n",
    "s0, s1 = 0, 0\n",
    "for item in P1:\n",
    "    s0 -= item * m.log2(item)\n",
    "\n",
    "for item in P2:\n",
    "    s1 -= item * m.log2(item)\n",
    "\n",
    "# Cambio en la incertidumbre\n",
    "Δs = - (s1 - s0)\n",
    "\n",
    "# Salida\n",
    "print(f\"|\", \"-\"*38, \"|\")\n",
    "print(f\"| Pos | letra | Inicial | Final | Cambio |\")\n",
    "print(f\"|\", \"-\"*38, \"|\")\n",
    "print(f\"|  1  |   Z   | {s0:7.2f} | {s1:5.2f} | {Δs:6.2f} |\")\n",
    "print(f\"|\", \"-\"*38, \"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Información\n",
    "---\n",
    "### 2.1. Definición:\n",
    "Es un conjunto de datos (mensaje) sobre un determinado fenómeno que no se conocían.\n",
    "\n",
    "Se basa en:\n",
    "* La información es transferida desde una fuente a un destinatario, solo si este ultimo no la conocía previamente.\n",
    "* La información siempre es mayor o igual a cero\n",
    "* No existe información en un símbolo seguro (probabilidad)\n",
    "* Existe información infinita en un símbolo imposible (probabilidad)\n",
    "\n",
    "### 2.2. Ecuaciones:\n",
    "\n",
    "\\begin{align*}\n",
    "I(s) &= - (S_f - S_o) \\quad \\text{Reducción de la incertidumbre} \\\\\n",
    "I(x_i) &= log_b \\frac {1}{p(x_i)} = - log_b p(x_i) \\quad \\text{Autoinformación} \\\\\n",
    "\\end{align*}\n",
    "\n",
    ">Nota:\n",
    "> * Si la base del logaritmo es $b=2$, la unidad de información es __bits__;\n",
    "> * Si la base es $b=e$, entonces la unidad es __nats__; \n",
    "> * Y finalmente si $b=10$, la unidad es __hartleys__.\n",
    "\n",
    "\n",
    "### <code>Ejemplo:</code> Alfabeto español\n",
    "Continuando con la distribución de probabilidades del ejemplo anterior:\n",
    "* ¿Cuál es la información asociada a la primera letra \"Z\" en un codificador optimo?\n",
    "* ¿Cuál es la información asociada a la llegada de la primera letra \"Z\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La información asociada a la letra Z en un codificador optimo es:      7.59 bits.\n",
      "La información asociada a la letra Z por reducción de incertidumbre:   1.84 bits.\n"
     ]
    }
   ],
   "source": [
    "auto_info_z = m.log2(1/probabilidades[\"Z\"])\n",
    "\n",
    "# Salida\n",
    "print(f\"La información asociada a la letra Z en un codificador optimo es:    {auto_info_z:6.2f} bits.\")\n",
    "print(f\"La información asociada a la letra Z por reducción de incertidumbre: {Δs:6.2f} bits.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Entropía\n",
    "---\n",
    "### 1.1. Definición:\n",
    "En teoría de la información la entropía es una medida de la incertidumbre de una fuente de información.\n",
    "\n",
    "Se basa en:\n",
    "* La entropía es positiva\n",
    "* La entropía es proporcional a la información \n",
    "* Si todos los símbolos de una fuente son igual de probables, entonces la entropía de la fuente será máxima.\n",
    "\n",
    "### 1.2. Ecuaciones:\n",
    "Dada una variable aleatoria $X = \\{x_1, x_2,...,x_i,...,x_n\\}$ la entropía nos da el número medio de bits (si usamos logaritmos de base 2) necesarios para codificar el mensaje a través de un codificador óptimo:\n",
    "\n",
    "\\begin{align*}\n",
    "H(X) &= \\sum_{i=1}^{n} p(x_i) \\cdot log_b \\frac {1}{p(x_i)} = - \\sum_{i=1}^{n} p(x_i) \\cdot log_b p(x_i) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "### 1.3. Propiedades:\n",
    "Algunas de las reglas que podemos mencionar:\n",
    "\n",
    "| Nombre | Ecuación | Nota |\n",
    "|--|--|:--:|\n",
    "| Entropía Conjunta | $ H(X,Y) = - \\sum_{i,j=1,1}^{n,m} p(x_i,y_j) \\cdot log_b p(x_i,y_j) $ | (1) |\n",
    "| Entropía Condicional | $ H(X/Y) = - \\sum_{j=1}^{m} p(y_j) \\sum_{i=1}^{n} p(x_i/y) \\cdot log_b p(x_i/y) $ | (2) |\n",
    "\n",
    "<p><small>\n",
    "<div><sup>1,2</sup> Para dos variables aleatorias X,Y dependientes entre si</div>\n",
    "</small></p>\n",
    "\n",
    "### <code>Ejemplo:</code> caja de laboratorio con resistencias\n",
    "Una caja \n",
    "* La probabilidad de que la primera fue de 50Ω y la segunda de 100Ω"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Mas Recursos\n",
    "\n",
    "- [Información](https://es.wikipedia.org/wiki/Informaci%C3%B3n) (Wikipedia)\n",
    "- [Entropía](https://es.wikipedia.org/wiki/Entrop%C3%ADa_(informaci%C3%B3n)) (Wikipedia)\n",
    "- [Teoría de la información](https://es.wikipedia.org/wiki/Teor%C3%ADa_de_la_informaci%C3%B3n) (Wikipedia)\n",
    "- [Ley de los grandes números](https://es.wikipedia.org/wiki/Ley_de_los_grandes_n%C3%BAmeros) (Wikipedia)\n",
    "- [Teorema del límite central](https://es.wikipedia.org/wiki/Teorema_del_l%C3%ADmite_central) (Wikipedia)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
